{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title:  Lightweight CNN for Text Classification with LSTM-Based Performance Enhancement\n",
    "\n",
    "#### Group Member Names :\n",
    "\n",
    " Kelvin Ikrokoto\n",
    "\n",
    " Clinton Avornu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION:\n",
    "Text classification plays a crucial role in Natural Language Processing (NLP), supporting applications such as document categorization, spam detection, and sentiment analysis.\n",
    "This project focuses on reproducing the methodology of the research paper “Light-Weighted CNN for Text Classification” and implementing an additional contribution to evaluate the model's performance under a modified architecture. The baseline model is a lightweight Convolutional Neural Network (CNN), and the contribution involves introducing an LSTM-based model for comparison.\n",
    "*********************************************************************************************************************\n",
    "#### AIM :\n",
    "- To reproduce a lightweight CNN text classification model from a selected research paper.\n",
    "\n",
    "- To introduce a significant contribution by developing an LSTM model for comparative analysis.\n",
    "\n",
    "- To evaluate and compare the performance of both models using accuracy as the primary metric.\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo:\n",
    "https://github.com/RituYadav92/Lightweighted-CNN-for-Document-Classification\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "The selected research paper proposes a lightweight CNN architecture designed to improve text classification efficiency while maintaining competitive performance. The paper emphasizes reducing model complexity, speeding up training, and maintaining classification accuracy across multiple classes.\n",
    "The author's implementation employs convolutional layers to extract local textual features and a pooling layer to reduce dimensionality, followed by dense layers for classification.\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "Traditional deep learning approaches for text classification often involve heavy architectures that require significant computational resources. The need for lightweight and efficient models is increasing, especially for deployment in low-resource environments.\n",
    "This project aims to reproduce the lightweight CNN proposed in the paper and evaluate whether incorporating a sequential model (LSTM) can further enhance classification performance.\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "Modern NLP systems must balance accuracy and efficiency. CNNs capture local text patterns effectively but may struggle with long-term dependencies. LSTMs, by contrast, are designed to capture sequential patterns.\n",
    "By comparing both architectures within the same dataset, this project explores trade-offs between lightweight design and sequence modeling.\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "- A lightweight CNN was implemented as the baseline model.\n",
    "\n",
    "- An LSTM model was introduced as the contribution to analyze the impact of sequential learning.\n",
    "\n",
    "- Both models were trained and evaluated on the same dataset for fair comparison.\n",
    "\n",
    "- Accuracy was used as the main evaluation metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "### Reference\n",
    "Light-Weighted CNN for Text Classification (Research Paper)\n",
    "### Explanation\n",
    "The study proposes a minimalistic CNN architecture aimed at reducing computational cost while achieving adequate multi-class classification performance. The architecture typically consists of embedding → convolution → pooling → dense layers.\n",
    "### Dataset/Input\n",
    "- Training Data: tobacco-data folder\n",
    "\n",
    "- Testing Data: sfsf_test_pol_data_bbf folder\n",
    "\n",
    "- Labels range from 0 to 9, where each .txt file corresponds to a class.\n",
    "### Weakness\n",
    "- CNN may not capture long-range dependencies.\n",
    "\n",
    "- Original implementation used TensorFlow 1.x, no longer compatible with modern environments.\n",
    "\n",
    "- Limited documentation on hyperparameters and dataset preprocessing.\n",
    "\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "*********************************************************************************************************************\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "cnn_model = Sequential([\n",
    "\n",
    "    Embedding(input_dim=vocab_size,\n",
    "              output_dim=embedding_dim,\n",
    "              input_length=max_len),\n",
    "    Conv1D(128, 5, activation=\"relu\"),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation=\"softmax\")  # 10 classes: 0–9\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "cnn_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkHhku9h-j2"
   },
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code :\n",
    "lstm_model = Sequential([\n",
    "\n",
    "    Embedding(input_dim=vocab_size,\n",
    "              output_dim=embedding_dim,\n",
    "              input_length=max_len),\n",
    "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "Baseline CNN Accuracy:\n",
    "\n",
    "41.48%\n",
    "\n",
    "LSTM Contribution Accuracy:\n",
    "\n",
    "42.99%\n",
    "\n",
    "The LSTM model outperformed the baseline CNN by approximately 1.51%, demonstrating that incorporating sequential modeling provides measurable performance improvement.\n",
    "*******************************************************************************************************************************\n",
    "\n",
    "\n",
    "#### Observations :\n",
    "- CNN trains faster but may miss long-term dependencies.\n",
    "\n",
    "- LSTM captures sequential information, leading to slightly better accuracy.\n",
    "\n",
    "- Dataset appears noisy, contributing to moderate accuracy scores for both models.\n",
    "\n",
    "- Increasing epochs or using hybrid CNN-LSTM architectures might further improve performance.\n",
    "*******************************************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3"
   },
   "source": [
    "### Conclusion and Future Direction :\n",
    "The project successfully addressed the key objectives: replicating the lightweight CNN for text classification and implementing an LSTM-based contribution. The LSTM model achieved a higher test accuracy than the CNN baseline, validating the significance of the contribution.\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings :\n",
    "- CNNs are efficient but limited in handling sequential data.\n",
    "\n",
    "- LSTMs improve contextual understanding in text classification tasks.\n",
    "\n",
    "- TensorFlow version compatibility is crucial when reproducing research code.\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "The LSTM model’s improved accuracy indicates that sequential dependencies play an important role in classification tasks, even when datasets contain short text fragments. The CNN baseline still provides competitive results with lower computational cost.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "- Dataset labels may not be perfectly balanced.\n",
    "\n",
    "- Training time increases significantly with LSTM.\n",
    "\n",
    "- Only one dataset was used for evaluation.\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n",
    "- Explore hybrid CNN-LSTM models.\n",
    "\n",
    "- Test transformer-based models such as BERT.\n",
    "\n",
    "- Apply hyperparameter tuning and regularization techniques.\n",
    "\n",
    "- Evaluate model generalization using additional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Light-Weighted CNN for Text Classification — Research Paper\n",
    "\n",
    "[2]:  Source Code Repository: https://github.com/RituYadav92/Lightweighted-CNN-for-Document-Classification\n",
    "\n",
    "[3]:  TensorFlow Documentation (Keras API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQnMSAf-h-j4"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (cass311)",
   "language": "python",
   "name": "cass311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
